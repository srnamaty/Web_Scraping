{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8626083-0541-4e81-b45f-a44f74dae7e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Web Scraping for Job Posting Data:\n",
    "_Author: Matěj Srna_\n",
    "***\n",
    "<img src=\"LOGO/webscraping.png\" style=\"width:640px; height:400px; margin-left: auto; margin-right: auto\" />\n",
    " \n",
    "> _**Disclaimer: This Web Scraping is only used for the educational purposes**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c3c46c-a3a2-4ece-8deb-89f1a03ee84a",
   "metadata": {},
   "source": [
    "#### Content:\n",
    "1. [Module imports](#Module-imports:)\n",
    "2. [List of http links to jobs](#List-of-http-links-to-jobs:)\n",
    "3. [Data scraping from the list of links](#Data-scraping-from-the-list-of-links:)\n",
    "4. [Creation of CSV file out of web scraping](#Creation-of-CSV-file-out-of-web-scraping:)\n",
    "5. [Creation of summary CSV file out of all CSV files](#Creation-of-summary-CSV-file-out-of-all-CSV-files:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a911c0-e019-44b8-ac8c-5ae9c59affea",
   "metadata": {},
   "source": [
    "### Module imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abaf1b-657a-40c0-881c-c47f71b6c9b1",
   "metadata": {},
   "source": [
    "***\n",
    "For this analysis, I had to import several libraries. Mainly I used BeautifulSoup this library uses a parser to help navigate and search for the key elements which I specified to get the necessary data.\n",
    "\n",
    "Secondly, I used pandas which are best for the data structuring. I used it to structure the data into DataFrame and also to save the data into CSV files.\n",
    "\n",
    "Then I used the request library to get data from the websites I decided to use this library to avoid the issues with JavaScript.\n",
    "\n",
    "Lastly, I used the DateTime library to import the actual date of scraping, the time library to slow down the speed of scraping, and the glob library to easily use paths to the files.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3524eab-7f34-47b2-8543-f6b22a13822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import date\n",
    "import datetime\n",
    "from time import sleep\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d47b0-8d8f-461c-a861-d7461d18e873",
   "metadata": {},
   "source": [
    "## List of http links to jobs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0920bbd8-fe50-4656-91a1-5a161bb8c3ab",
   "metadata": {},
   "source": [
    "***\n",
    "Here is the list of 4 links to the search pages. Each page contains 25 job postings. This is the limitation of LinkedIn, therefore I used this list to iterate and get the links to actual job postings. As you can see, at the end of each link there is __*\"&start=\"*__ with number specification. Each page contains 25 jobs. In the list called links should be up to 100 links to the job posts.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11aa9eb9-23e5-4767-b67b-12fda8ddcc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_browser = [\"https://www.linkedin.com/jobs/search/?f_WT=3&keywords=data%20scientist&location=%C4%8Desko\", \n",
    "                \"https://www.linkedin.com/jobs/search/?f_WT=3&keywords=data%20scientist&location=%C4%8Desko&start=25\", \n",
    "                \"https://www.linkedin.com/jobs/search/?f_WT=3&keywords=data%20scientist&location=%C4%8Desko&start=50\", \n",
    "                \"https://www.linkedin.com/jobs/search/?f_WT=3&keywords=data%20scientist&location=%C4%8Desko&start=75\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "322ceb27-e1f2-4d9d-a1b6-06986e23851c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size_of_list = len(link_browser)\n",
    "\n",
    "links = []\n",
    "\n",
    "#getting the links to the job_postings\n",
    "for n in range(size_of_list):\n",
    "    response = requests.get(link_browser[n])\n",
    "    html_text = response.text\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    hrefs = soup.find_all(\"a\", class_= \"base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]\")\n",
    "    for n in hrefs:\n",
    "        links.append(n[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e18b31bb-7466-487c-b0ea-107e37d09899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f9fa7-9d63-45b4-8bd7-dddc5daaab13",
   "metadata": {},
   "source": [
    "## Data scraping from the list of links:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7a3760-793f-4f3b-a662-fd7474561454",
   "metadata": {},
   "source": [
    "***\n",
    "This part of the code consists of several parts:\n",
    " * Creation of dictionary which specifies column names with lists as values\n",
    " * Looping through each link searching for the element on the website \n",
    " * Appending the searched information to the dictionary's lists \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7c282c6-2b0c-4faa-bd06-959923e28c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Webscraping 73 of job postings: 100%|██████████████████████████████████████████████████| 73/73 [01:15<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# To get all the data and avoid any errors with javascript loading through the request module,\n",
    "# I use the number of links in the list of links. If the scraping is successful the variable n is increased.\n",
    "# If the scraping was not successful, the variable n remains the same. \n",
    "# This scraping keeps going until the code scraped all links.\n",
    "\n",
    "size_of_list = len(links)\n",
    "\n",
    "# For adding the data into pandas dataframe I used lists nested in dictionary,\n",
    "# which will be inserted together.\n",
    "d = {\n",
    "    \"name\": [],\n",
    "    \"company_name\": [],\n",
    "    \"ago\": [],\n",
    "    \"contract\": [],\n",
    "    \"location\": [],\n",
    "    \"date\": [],\n",
    "    \"description\": [],\n",
    "    \"link\": [],\n",
    "    }\n",
    "\n",
    "\n",
    "n = 0\n",
    "\n",
    "# This code combines the request module to get data and also soup,\n",
    "# which parsers through the data looking for particular tags.\n",
    "with tqdm(total=size_of_list) as pbar:\n",
    "    pbar.set_description(\"Webscraping %s of job postings\" % size_of_list)\n",
    "    while n < size_of_list:\n",
    "        try:\n",
    "            response = requests.get(links[n])\n",
    "            html_text = response.text\n",
    "\n",
    "            soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "            try:\n",
    "                name_s = soup.find(\"div\", class_ =\"top-card-layout__entity-info-container flex flex-wrap papabear:flex-nowrap\")\n",
    "                name_s = name_s.find(\"h1\", class_=\"top-card-layout__title font-sans text-lg papabear:text-xl font-bold leading-open text-color-text mb-0 topcard__title\")\n",
    "            except:\n",
    "                name_s = soup.find(\"div\", class_ =\"top-card-layout__card relative p-2 papabear:p-details-container-padding\")\n",
    "                name_s = name_s.find(\"div\", class_=\"top-card-layout__entity-info flex-grow flex-shrink-0 basis-0 babybear:flex-none babybear:w-full babybear:flex-none babybear:w-full\")\n",
    "                #name_s = name_s.find(\"h1\", class_=\"top-card-layout__title font-sans text-lg papabear:text-xl font-bold leading-open text-color-text mb-0 topcard__title\")\n",
    "\n",
    "            com_name_s = soup.find(\"span\", class_ =\"topcard__flavor\")\n",
    "            time_s = soup.find(\"span\", class_=\"posted-time-ago__text topcard__flavor--metadata\")\n",
    "            description_s = soup.find(\"div\", class_= \"show-more-less-html__markup show-more-less-html__markup--clamp-after-5\") \n",
    "            contract_s = soup.find(\"span\", class_=\"description__job-criteria-text description__job-criteria-text--criteria\")\n",
    "            location_s = soup.find(\"span\", class_=\"topcard__flavor topcard__flavor--bullet\")\n",
    "            try:\n",
    "                salary_s = soup.find(\"div\", class_=\"salary compensation__salary\")\n",
    "            except:\n",
    "                pass\n",
    "            # Replacing the symbol (++), which causes the error in datacleaning.\n",
    "            try:\n",
    "                name = name_s.text.strip().replace(\"++\", \"+\")\n",
    "            except:\n",
    "                name = name_s.get_text.strip().replace(\"++\", \"+\")\n",
    "            company_name = com_name_s.text.strip()\n",
    "            try:\n",
    "                ago = time_s.get_text().strip()\n",
    "            except:\n",
    "                time_s = soup.find(\"span\", class_=\"posted-time-ago__text posted-time-ago__text--new topcard__flavor--metadata\")\n",
    "                ago = time_s.get_text().strip()\n",
    "            description = description_s.text.strip()\n",
    "            date = date.today()\n",
    "            contract = contract_s.text.strip()\n",
    "            location = location_s.text.strip()\n",
    "            try:\n",
    "                salary = salary_s.text.strip()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            d[\"name\"].append(name)\n",
    "            d[\"company_name\"].append(company_name)\n",
    "            d[\"ago\"].append(ago)\n",
    "            d[\"contract\"].append(contract)\n",
    "            d[\"description\"].append(description)\n",
    "            d[\"date\"].append(date)\n",
    "            d[\"link\"].append(links[n])\n",
    "            d[\"location\"].append(location)\n",
    "\n",
    "            # function sleep to slow down the speed of webscraping\n",
    "            sleep(0.3)\n",
    "\n",
    "            # updating the variable n to determine when the while loop should break\n",
    "            n += 1\n",
    "            # updating the tqdm progression bar\n",
    "            pbar.update(1)\n",
    "        except AttributeError:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca9ff2-4f67-4e7d-bc03-9b7c7f6ca5a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> To see the progress of web-scraping I used tqdm bar to show progress.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ac2d0-364a-4799-85c0-ce0cd6fe6587",
   "metadata": {},
   "source": [
    "## Creation of CSV file out of web scraping: \n",
    "***\n",
    "Here I create the separated CSV files with data created by the Pandas library. Each scraping is saved into a separate file with the date included in the file name. Therefore I can keep the data separated in case of any error and I can easily restore the data. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47e67001-11b2-4b73-80cd-eae0e1709fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data from the dictionary into data frame and transposing it to the table layout\n",
    "data = pd.DataFrame.from_dict(data=d, orient='index')\n",
    "data = data.transpose()\n",
    "\n",
    "data.to_csv(f\"RAW_DATA/scraped_data_{date}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa010ec-3cf6-4715-ba4e-a17fc71b8cda",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Example:</b> The name of the file is always scraped_data_*date*.csv. Therefore, scraped_data_2022-05-15.csv or scraped_data_2022-05-16.csv\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e0bdf-5eff-400e-9ff6-05f50f15d7a9",
   "metadata": {},
   "source": [
    "## Creation of summary CSV file out of all CSV files:\n",
    "***\n",
    "In this part of the code, I merge all the CSV files which have been already scraped. For further analysis and Data cleaning, I needed to put all the data into one file. For this I use Pandas DataFrame. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4532f098-b622-41da-a1b6-7d068c65e252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2282, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data file names\n",
    "path =r\"C:\\Users\\srnam\\PycharmProjects\\data_analyz_pandas\\LinkedIn_job_analysis\\RAW_DATA\"\n",
    "filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    dfs.append(pd.read_csv(filename))\n",
    "\n",
    "# Concatenate all data into one DataFrame\n",
    "big_frame = pd.concat(dfs, ignore_index=True)\n",
    "big_frame.to_csv(\"DATA/summary_data.csv\", index=False)\n",
    "big_frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a17d7-ea38-4683-bb66-f3fb476dc7f1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> To get all file names, I used the glob module to get all separated files with scraped data.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf78140-0967-4d7b-b697-d6a1d31ab84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = soup.find()\n",
    "#print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8928d-c353-4ea2-a7e0-7296ef37687c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f667040e-d627-466a-930e-12eb980a389d",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<span style=\"color:green\"><p style=\"text-align:right; font-style: italic\"><b>Matěj Srna</b></p></span>\n",
    "<hr></hr>\n",
    "<em>My links:</em>\n",
    "<br>\n",
    "<table style=\"float:left; width: 250px; border-collapse: separate;\">\n",
    "<thead>\n",
    "<tr><th><a href=\"https://www.linkedin.com/in/matejsrna\"><img src=\"LOGO/linkdin.png\" style=\"width:48px; height:48px\" /></a></th><th><a href=\"https://github.com/srnamaty\"><img src=\"LOGO/git.png\" style=\"width:58px; height:58px\" /></a></th><th><a href=\"https://www.srnamatej.com\"><img src=\"LOGO/website.png\" style=\"width:58px; height:58px\" /></a></th></tr>\n",
    "</thead>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
